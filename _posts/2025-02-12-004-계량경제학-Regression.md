---
title: 004-계량경제학 회귀 분석(Regression)
date: 2025-02-12 01:13:00 +0900
categories: [계량경제학, Regression]
tags: [econometric, regression, 회귀 분석, lsm, lad, r square, error]
author: rachihyeon 
description: 회귀 분석에 대한 포스트 입니다.
# comments: 
# media_subpath: 
# pin: true
math: true
mermaid: true
---

><span style="font-size: 20px;">**!!필독!!** <br>
>수식편집기의 버그로 수식 번호가 모두 매겨져 있는 경우가 있습니다. <br>
>이 경우 **새로고침(F5)**하여 없앤 후 읽어주시길 바랍니다.</span>
{: .prompt-danger}

<br>

이번에는 드디어 회귀 분석에 대해서 다룹니다.

[계량경제학 포스트 보러가기](/categories/계량경제학/)

## 0. Prerequisite & Introduction
선행 지식으로는 선형대수학의 행렬 부분이 필요합니다.

질문 : 자녀는 정상재(Normal goods)인가 열등재(Inferior goods)인가?<br>
답 : 소득에 따라 그래프를 그려보면 감소하는 경향을 보인다. 즉, 소득이 많다고 해서 자녀의 수가 느는 것은 아니라는 것이다. 따라서 답은 열등재이다.
![Child normal or inferior](/assets/img/post_img/econometrics/child_normal_inferior_graph.png)

질문을 바꿔보자,

질문 : 자녀 한 명당 지출은 정상재인가 열등재인가?<br>
답 : 소득에 따라 자녀 한 명당 지출은 증가하는 그래프를 그려볼 수 있다. 따라서 정상재이다.
![Child normal of inferior 2](/assets/img/post_img/econometrics/child_normal_inferior_graph_2.png)

<br>

위 두 방식처럼 어떤 변수에 대해서 목적 변수가 어떻게 변하는지를 분석하고 존재하지 않는 input에 대해서 output을 예측하기 위해서는 어떤 관계가 있는지 알아볼 필요가 있다.

이것을 하는 방법 중 하나가 ***선형 회귀 분석(Linear regression)***이다.

<br>
<br>
<br>

## 1. 선형 회귀 분석(Linear regression)

예를 들어, 정상재 열등재인지를 구분하는 모형을 만들려 하면 소비와 수입 간의 관계를 알아야 하니, 소비를 $$C$$, 소득을 $$I$$라고 하면 $$C=a+bI+\epsilon$$라고 쓸 수 있다.

>이때 $$\epsilon$$은 오차이다. 왜냐하면 소득이 소비를 완전히 설명할 수는 없기 때문이다. <br>정말 그렇다면 $$\epsilon = 0$$라고 나오겠지만, 우선 쓴다.
{: .prompt-info}

우리의 목표는 소득으로 소비를 최대한 설명해야하니 오차를 최소화 하는 것으로 대체할 수 있다. 따라서 우리의 목표는 $$\min _{a,\ b}^{ } \epsilon$$이라고 할 수 있다.

<br>
<br>

## 2. 최적화 목적 함수 LSM, LAD(Least Squared Method, Least Absolute Deviations)

위 섹션에서 말했다시피 우리는 $$\epsilon$$을 가장 작게 만들어야 한다. $$\epsilon$$에 대한 식을 세우고 미분해서 임계점 중 최솟값을 찾으면 되니 우리는 적당한 두 가지 방법을 찾을 수 있다.

모델을 다시 세워보면, $$\epsilon_t = C_t-a-bI_t$$이고, 아래첨자의 경우 각 데이터에 대한 것이다.

단순히 $$\epsilon_t$$을 모두 더하면 그 값이 $$-\infty$$가 되도록 $$a, b$$가 조정될 것이기 때문에 해를 찾을 수 없다.<br>
따라서 $$\epsilon_t$$에 적당한 연산을 취해 양수로 만들어 그 합이 $$0$$에 가까워지도록 만들면 되는 것이다. 

그 방법 중 하나가 LSM(Least Squared Method), 다른 하나가 LAD(Least Absolute Deviations)이다.<br>
수식으로 작성하면 아래와 같다.

$$
\begin{align}
&\min _{ }^{ } \sum _{\ t}^{\ }\left(\epsilon _t\right)^2=\min _{ }^{ }\sum _{\ t}^{\ }\left(C_t-a-bI_t\right)^2 \\
&\min _{ }^{ } \sum _{\ t}^{\ }\left| \epsilon _t\right|=\min _{ }^{ }\sum _{\ t}^{\ }\left| {C_t-a-bI_t}\right| \\
\end{align}
$$

### 2-1. 이상치(Outlier)

위 두 모델은 이상치에 받는 영향에 차이가 있다.<br>
우리가 수집한 데이터에서 이쁘게 혹은 잘 혹은 보기 좋게 존재하는게 아니라 한두 가지 동떨어진 데이터가 존재할 수 있는데 이 동떨어진 데이터를 **이상치(Outlier)**라고 한다.

LSM모델의 경우 제곱을 하기에 편차가 큰 데이터에 영향을 많이 받는다. 즉, 이상치에 영향을 크게 받는다고 할 수 있다.<br>
반면, LAD모델의 경우 모든 데이터가 동일한 가중치를 부여받기 때문에 이상치의 영향이 적다고 할 수 있다.

>가중치가 같다는 이야기를 했는데 $$\epsilon$$자체에 결합 되는 weight가 아니라 모든 데이터가 목적함수에 동일하게 영향을 미친다는 뜻이다.
{: .prompt-warning}

>사실 필자는 전공이 컴퓨터공학이라 LSM을 L-2 norm, LAD를 L-1 norm이라 쓸 수도 있는데 같은 말이니 혼동 없길 바랍니다. <br>
>추가로 L-1norm은 Manhattan distance라고 쓸 때도 있을 겁니다.
{: .prompt-tip}

<br>
<br>

## 3. 목적 함수 최적화

목적 함수를 최적화 하기 전에 잠깐 오차항($$epsilon$$)에 대해서 다뤄보겠다.

### 3-1. 오차항(Error)

오차항이 무엇인지 우리가 세운 식에서 알아보자.

$$
C=a+bI+\epsilon
$$

에서 우리가 모형을 세웠을 때 한 [가정](#1-선형-회귀-분석linear-regression)에 따르면, $$\epsilon$$은 모델에서 $$I$$으로 설명되지 않는 부분이라고 했다.<br>
따라서 $$Y=E[Y|X]+(Y-E[Y|X])=E[Y|X]+\epsilon$$라고 할 수 있다. <br>
양 변에 $$X$$에 대한 조건부 기댓값을 취하면, $$E[\epsilon|X]=0=E[E[\epsilon|X]]$$이 성립한다.

>The law of iterated expectations에 의해 $$E[\epsilon]=E[E[\epsilon|X]]$$이 만족되며,<br>
>$$E[\epsilon]=0$$이기 때문에 위 식이 성립된다.
{: .prompt-info}

<br>

### 3-2. Projection and regression

확률 변수 $$X, Y$$에 대한 이변량 분포 함수를 $$f(X, Y)$$라 하자.<br>
이 두 변수에 대해서 선형 회귀 분석을 하기 위해 세운 모델을 $$Y=\alpha + \beta X+\epsilon\ \ \ where\ E[\epsilon|X]=0$$라 할 때,<br>

$$
\begin{align}
&Cov(X, Y)=Cov(X, \alpha)+\beta Cov(X, X)+Cov(X, \epsilon)=0+\beta Cov(X,X)+0 \\
&\therefore \beta=\frac{Cov(X,Y)}{Cov(X,X)}=\frac{Cov(X,Y)}{V(X)} \\
&and, \alpha=E[Y]-\beta E[X]-E[\epsilon]=E[Y]-\beta E[\epsilon|X]=E[Y]-\beta E[X] \\
\end{align}
$$

우리는 위 방식으로부터 $$\alpha, \beta$$값을 얻어낼 수 있다.<br>
그렇다면, $$E[Y|X]=\alpha+\beta X$$를 의미하는가?<br>
답은 아니다. 그저 $$X$$에 대한 $$Y$$의 사영(projection)일 뿐이다. 왜냐하면, True 모델이 선형 모델이 아닐 수 있기 때문이다.

하나 더, $$X, Y$$가 독립인지에 대해서 알아볼 필요가 있다. 우리가 세운 모델에 따르면 $$X=\gamma Y+\eta$$로 쓸 수 있으며, <br>
우리는 여기서도 $$\gamma, \eta$$값을 얻어낼 수 있다. 따라서 단순 선형 회귀 모형에서는 공분산(Covariance)와 밀접한 관계가 있게 되는 것이다.

### 3-3. Optimization

위에서 우리는 모형 $$Y_t=\alpha + \beta X_t+\epsilon_t$$를 세웠다. 이 모델로 LSM모델로 최적화 과정을 진술하겠다.

$$
\begin{align}
&\min _{ }^{ }Q=\min _{ }^{ }\sum _{t\ }^{\ }\left(Y_t-\alpha -\beta X_t\right)^2 \\ 
\end{align}
$$

1계 조건에 의해,

$$
\begin{align}
    &\frac{\partial Q}{\partial \alpha }=2\sum _{t\ }^{\ }\left(Y_t-\alpha -\beta X_t\right)\left(-1\right)=0 \tag{1} \\
    &\frac{\partial Q}{\partial \beta }=2\sum _{t\ }^{\ }\left(Y_t-\alpha -\beta X_t\right)\left(-X_t\right)=0 \tag{2}
\end{align}
$$

<br>

$$
\begin{align}
&(1) 로부터 \\
&\sum _{t\ }^{\ }\left(Y_t-\alpha -\beta X_t\right)=\sum _{t\ }^{\ }Y_t-T\alpha -\beta \sum _{t\ }^{\ }X_t\\ \Rightarrow 
&\hat{\alpha }=\frac{1}{T}\sum _{t\ }^{\ }Y_t-\hat{\beta }\sum _{t\ }^{\ }X_t=\overline {Y}-\hat{\beta} \overline {X} \tag{3} 
\end{align}
$$

$$
\begin{align}
&(2), (3) 로부터 \\
&\frac{\partial Q}{\partial \beta }=2\sum _{t\ }^{\ }\left(Y_t-\alpha -\beta X_t\right)\left(-X_t\right)=2\sum _{t\ }^{\ }\left(Y_t-\overline {Y}+\beta \overline {X}-\beta X_t\right)\left(-X_t\right)\\ 
&\Rightarrow \hat{\beta} = \frac{\sum _{t\ }^{\ }\left(Y_t-\overline {Y}\right)\left(X_t\right)}{\sum _{\ t}^{\ }\left(X_t-\overline {X}\right)\left(X_t\right)}=\frac{\sum _{t\ }^{\ }\left(Y_t-\overline {Y}\right)\left(X_t-\overline {X}\right)}{\sum _{\ t}^{\ }\left(X_t-\overline {X}\right)\left(X_t-\overline {X}\right)}=\frac{S_{XY}}{S_{XX}}=\frac{Cov(X,Y)}{Var(X)}
\end{align}
$$

결과를 보니 [사영을 통한 방식](#3-2-projection-and-regression)의 결과와 동일한 것을 볼 수 있다.(상당히 흥미롭다.)

추가적으로, $$\hat{\beta }=\frac{S_{XY}}{S_{XX}}=\frac{S_{XY}}{\sqrt{S_{XX}S_{YY}}}\frac{\sqrt{S_{YY}}}{\sqrt{S_{XX}}}=\rho \left(X,Y\right)\frac{\sigma _Y}{\sigma _X}$$가 만족한다. <br>
이로부터 우리는 $$\hat{\beta}$$는 unit free하다는 것은 알 수 있다.

$$X, Y$$에 대한 그래프를 그려보면,<br>
![Beta estimator graph](/assets/img/post_img/econometrics/beta_estimator_graph.png)
위 이미지로부터 우리가 얻은 $$\beta$$에 대한 추정치 $$\hat{\beta}$$은 $$Y$$를 설명하기 위해 필요한 변수인 $$X$$에 결합되는 ***weight***임을 더 직관적으로 볼 수 있다.

<br>

### 3-4. 기울기 가중치를 사용한 이해

위 섹션에서 얻은 $$\hat{\beta}$$에서 간단한 식 조작을 하면 아래와 같다.

$$
\begin{align}
\hat{\beta }=\frac{\sum _{t\ }^{\ }\left(Y_t-\overline {Y}\right)\left(X_t\right)}{\sum _{\ t}^{\ }\left(X_t-\overline {X}\right)\left(X_t\right)}=\frac{\sum _{t\ }^{\ }\left(Y_t-\overline {Y}\right)\left(X_t-\overline {X}\right)}{\sum _{\ t}^{\ }\left(X_t-\overline {X}\right)^2}=\sum _{\ t=1}^{\ T}\frac{\left(X_t-\overline {X}\right)^2}{\sum _{j=1\ }^{\ T}\left(X_j-\overline {X}\right)^2}\cdot \frac{\left(Y_t-\overline {Y}\right)}{\left(X_t-\overline {X}\right)}
\end{align}
$$

여기서 마지막 항인 $$\frac{\left(Y_t-\overline {Y}\right)}{\left(X_t-\overline {X}\right)}$$은 평균으로부터 t점까지 이은 선분의 기울기이다.<br>
따라서, $$\hat{\beta}$$은 $$\frac{\left(Y_t-\overline {Y}\right)}{\left(X_t-\overline {X}\right)}$$의 가중 평균이며, 이때 가중치는 $$\frac{\left(X_t-\overline {X}\right)^2}{\sum _{j=1\ }^{\ T}\left(X_j-\overline {X}\right)^2}$$이다.

위 관점으로 이상치의 영향을 이해해보면, 이상치는 기댓값으로부터 다른 데이터들과 다르게 동떨어져 있기 때문에 가중치가 큰 값을 가져 전체 모델에 영향을 주게 되는 것이다.<br>
실제 데이터 분석에서는 이상치를 제거하거나 알고리즘을 도입하여 가중치를 교정하는 과정이 필요하다.

>여담. 필자는 계량경제학을 공부할 때 이 섹션에서 설명하는 관점이 가장 흥미로웠다.
{: .prompt-info}

>우리가 세운 모델의 $$\alpha, \beta$$는 $$X, Y$$의 관계를 완벽하게 설명하는 값이고 우리가 얻은 $$\hat{\alpha}, \hat{\beta}$$는 우리의 데이터로 얻을 수 있는 가장 기대 해 볼 만한 값이다.
>따라서 두 값이 일치하지 않을 수 있다. <br>
>우리가 얻은 $$\hat{\alpha}, \hat{\beta}$$을 통해 $$Y$$의 값을 예측할 수 있는데 이 예측값을 $$\hat{Y}$$라 하고 $$\hat{Y_t}=\hat{\alpha}+ \hat{\beta}X_t$$이다.<br>
>그리고 당연히 $$\hat{Y}$$값과 $$Y$$은 다른 값이다. 그 차이는 $$Y_t-\hat{Y_t}=e_t$$라고 하며 잔차(Residual)이라고 부른다.
{: .prompt-tip}

<br>
<br>

## 4. 결정 계수 $$R^2$$

우리는 모델의 계수의 추정치를 계산하는 방법을 이야기 했다. 그렇다면 우리가 얻은 계수가 얼마나 모델을 잘 설명하는지에 대한 지표가 필요하다.<br>
그 방법 중 하나가 바로 $$R^2$$값 이다.

$$R^2$$값은 모델이 $$Y$$의 variations를 얼마나 설명할 수 있는지에 대한 지표이다.<br>
계산식은 다음과 같다.

$$
\begin{align}
T\cdot V\left(Y\right)&=\sum _{t\ }^{\ }\left(Y_t-\overline {Y}\right)^2=\sum _{t\ }^{\ }\left(Y_t-\hat{Y_t}+\hat{Y_t}-\overline {Y}\right)^2\\ 
&=\sum _{t\ }^{\ }\left\{\left(Y_t-\hat{Y_t}\right)^2+\left(\hat{Y_t}-\overline {Y}\right)^2+2\left(Y_t-\hat{Y_t}\right)\left(\hat{Y_t}-\overline {Y}\right)\right\}\\ 
&=\sum _{t\ }^{\ }\left(Y_t-\hat{Y_t}\right)^2+\sum _{t\ }^{\ }\left(\hat{Y_t}-\overline {Y}\right)^2+2\sum _{t\ }^{\ }\left(Y_t-\hat{Y_t}\right)\left(\hat{Y_t}-\overline {Y}\right)\\ 
&=\sum _{t\ }^{\ }e_t^2+\sum _{t\ }^{\ }\left(\hat{Y_t}-\overline {Y}\right)^2+2\sum _{t\ }^{\ }e_t\left(\hat{Y_t}-\overline {Y}\right)\\ 
&=\sum _{t\ }^{\ }e_t^2+\sum _{t\ }^{\ }\left(\hat{Y_t}-\overline {Y}\right)^2+2\sum _{t\ }^{\ }e_t\hat{Y_t}\ \ \ \left(\because \sum _{t\ }^{\ }e_t=0\right)\\ 
&=\sum _{t\ }^{\ }e_t^2+\sum _{t\ }^{\ }\left(\hat{Y_t}-\overline {Y}\right)^2+2\sum _{t\ }^{\ }e_t\left(\hat{\alpha }+\hat{\beta }X_t\right) \\
&=\sum _{t\ }^{\ }e_t^2+\sum _{t\ }^{\ }\left(\hat{Y_t}-\overline {Y}\right)^2+2\hat{\beta }\sum _{t\ }^{\ }e_tX_t\\ 
&=\sum _{t\ }^{\ }e_t^2+\sum _{t\ }^{\ }\left(\hat{Y_t}-\overline {Y}\right)^2\ \ \ \ \left(\because \sum _{t\ }^{\ }e_tX_t=0\ \right)
\end{align}
$$

이고 $$\sum _{t\ }^{\ }\left(Y_t-\overline {Y}\right)^2$$은 $$TSS$$(Total Sum of Square), $$\sum _{t\ }^{\ }\left(\hat{Y_t}-\overline {Y}\right)^2$$는 $$ESS$$(Explained Sum of Square), $$\sum _{t\ }^{\ }e_t^2$$는 $$USS$$(Unexplained Sum of Square)이라고 한다.<br>

$$R^2$$의 정의는 $$R^2=\frac{ESS}{TSS}=1-\frac{USS}{TSS}$$이고 0과 1 사이의 값을 가지게 된다. <br>
$$R^2$$의 의미는 전체 variation에서 설명가능한 variation의 비율이다. 또, 결정계수는 상관계수의 제곱과 같다.(간단히 유도 가능하니 생략하겠다.)

<br>

### 4-1. Adjusted $$R^2$$

위에서 $$R^2$$를 정의했는데, 이 식으로 계산된 값의 특징은 설명 변수($$X$$)의 수가 증가함에 따라 증가하는 경향을 보인다. 즉, 설명 변수의 신뢰성이 떨어진다. <br>
이를 방지하기 위해 조정된 결정계수(Adjusted $$R^2$$)가 있다.

식으로 작성하면 다음과 같다.

$$Adjusted\ R^2=1-\frac{USS\div \left(n-k-1\right)}{TSS\div \left(n-1\right)}$$

이때, $$n$$은 데이터의 수, $$k$$는 설명 변수의 개수이다.<br>
설명 변수가 새로 추가되면 조정된 결정계수의 값이 작아지는데, 그 설명 변수가 충분히 설명력을 가지면 값이 커지므로 설명 변수의 설명력을 확인할 수 있는 지표가 되는 것이다.

>1. $$n=1$$은 분석 자체가 무의미하니 고려하지 않는다. 
>2. $$n<k$$이면, 데이터로 선형계를 만들 수 있으나 해가 무한하여 분석이 불가하다. (구체적으로, 선형계의 $$rank$$가 $$k$$보다 작을 때 이다.)
{: .prompt-warning}

<br>
<br>

## 5. Multiple Regression

우리는 이때까지 일변수 모델을 가정하고 분석했다.<br>
여기에서는 다변수 모델링을 가정하여 계수를 계산해보겠다.

$$
\begin{align}
    &Y_t=\beta _0^*+\beta _1^*X_{t1}+\beta _2^*X_{t2}+...+\beta _{k-1}^*X_{t\left(k-1\right)}+\epsilon _t\\ 
    &\min _{ }^{ }Q=\min _{ }^{ }\sum _{t\ }^{\ }\left(Y_t-\beta _0^*-\beta _1^*X_{t1}-\beta _2^*X_{t2}-...-\beta _{k-1}^*X_{t\left(k-1\right)}\right)^2\\ 
    &Y_t=X_t'\beta ^*+\epsilon _t\ \ \ where\ \beta ^*= 
    \begin{bmatrix}
        \beta _0^* \\
        \beta _1^* \\
        \vdots \\
        \beta_{k-1}^* \\
    \end{bmatrix}
    , X_t = 
    \begin{bmatrix}
        1 \\
        X_{t1} \\
        \vdots \\
        X_{t(k-1)} \\
    \end{bmatrix}
\end{align}
$$

$$
\begin{align}
&\min _{ }^{ }Q=\min _{ }^{ }\sum _{t\ }^{\ }\left(Y_t-X_t'\beta \right)^2\\ 
&\frac{\partial Q}{\partial \beta }=2\sum _{t\ }^{\ }\left(Y_t-X_t'\beta \right)\left(-1\right)X_t=0\\ 
&\Rightarrow \sum _{t\ }^{\ }X_tY_t=\sum _{t\ }^{\ }X_tX_t'\beta 
&\Rightarrow \hat{\beta }=\left(\sum _{t\ }^{\ }X_tX_t'\right)^{-1}\sum _{t\ }^{\ }X_tY_t\\ 
\end{align}
$$

$$X_t$$를 사용하지 않고 모든 데이터를 $$X$$라고 하면,

$$
\begin{align}
&\min _{ }^{ }Q=\min _{ }^{ }\left(Y-X\beta \right)'\left(Y-X\beta \right)=\min _{ }^{ }\left\{Y'Y-Y'X\beta -\beta 'X'Y-\beta 'X'X\beta \right\}\\ 
&\frac{\partial Q}{\partial \beta }=-X'Y+2X'X\beta =0\\ 
&\hat{\beta }=\left(X'X\right)^{-1}X'Y
\end{align}
$$

>앞으로는 일변수든 다변수든 위 방식으로 표현할 예정이다.
{: .prompt-info}

<br>
<br>

후기) 쓸 내용이 정말 많긴 하네요... 앞으로 갈 길이 먼데 막막합니다...

***오타 혹은 잘못된 정보가 있다면 댓글 이메일 등등으로 알려주시면 감사하겠습니다. (꾸벅)***